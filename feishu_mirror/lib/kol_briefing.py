"""KOL è§‚ç‚¹æ—¥æŠ¥ â€” æœç´¢ã€æŠ“å–ã€æ‘˜è¦ã€ç»„è£…é£ä¹¦å¡ç‰‡ã€‚"""
from __future__ import annotations

import json
import logging
import os
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

logger = logging.getLogger(__name__)

CONFIG_PATH = Path(__file__).resolve().parent.parent / "kol_config.json"

# â”€â”€ Data types â”€â”€


@dataclass
class KolConfig:
    id: str
    name: str
    title: str
    search_queries: list[str]
    category: str  # "crypto" | "tech"
    enabled: bool = True


@dataclass
class KolArticle:
    url: str
    title: str
    snippet: str  # search snippet (fallback)
    content: str = ""  # scraped full text
    source_type: str = "web"  # "web" | "youtube"


@dataclass
class KolSummary:
    kol: KolConfig
    points: list[str] = field(default_factory=list)  # æ ¸å¿ƒè§‚ç‚¹
    sources: list[str] = field(default_factory=list)  # source URLs
    source_types: list[str] = field(default_factory=list)  # "web" | "youtube"
    error: str | None = None


# â”€â”€ Config â”€â”€


def load_kol_config() -> tuple[list[KolConfig], dict]:
    """Load KOL list + settings from JSON config."""
    with open(CONFIG_PATH, encoding="utf-8") as f:
        raw = json.load(f)

    kols = []
    for k in raw.get("kols", []):
        if not k.get("enabled", True):
            continue
        kols.append(KolConfig(
            id=k["id"],
            name=k["name"],
            title=k.get("title", ""),
            search_queries=k.get("search_queries", [k["name"]]),
            category=k.get("category", ""),
        ))

    settings = raw.get("settings", {})
    return kols, settings


# â”€â”€ Search + Scrape â”€â”€


def _search_with_retry(searcher, query: str, count: int) -> dict:
    """Search with 24h â†’ weekly fallback + retry on error."""
    import time
    result = searcher.search(query, count=count, freshness="pd")
    if not result.get("results") and not result.get("error"):
        result = searcher.search(query, count=count, freshness="pw")
    if result.get("error"):
        time.sleep(2)
        result = searcher.search(query, count=count, freshness="pw")
    return result


# Sites to auto-search per KOL (query appended with site: filter)
_AUTO_SEARCH_SITES = ["youtube.com"]


def fetch_kol_articles(
    kol: KolConfig,
    searcher,
    settings: dict,
) -> list[KolArticle]:
    """Search DuckDuckGo + YouTube + scrape top articles for one KOL."""
    from lib.web_reader import fetch_page

    max_results = settings.get("max_search_results_per_query", 5)
    max_scrape = settings.get("max_scrape_per_person", 3)
    max_chars = settings.get("scrape_max_chars", 6000)

    seen_urls: set[str] = set()
    articles: list[KolArticle] = []

    # 1. Normal web search
    for query in kol.search_queries:
        result = _search_with_retry(searcher, query, max_results)
        for item in result.get("results", []):
            url = item.get("url", "")
            if not url or url in seen_urls:
                continue
            seen_urls.add(url)
            articles.append(KolArticle(
                url=url,
                title=item.get("title", ""),
                snippet=item.get("description", ""),
            ))

    # 2. Auto site-specific search (YouTube, etc.)
    for site in _AUTO_SEARCH_SITES:
        site_query = f"{kol.name} site:{site}"
        result = _search_with_retry(searcher, site_query, 3)
        for item in result.get("results", []):
            url = item.get("url", "")
            if not url or url in seen_urls:
                continue
            seen_urls.add(url)
            articles.append(KolArticle(
                url=url,
                title=item.get("title", ""),
                snippet=item.get("description", ""),
                source_type="youtube" if "youtube.com" in url else "web",
            ))

    # Scrape top N articles (skip YouTube â€” Playwright can't extract video content)
    scraped = 0
    for article in articles:
        if scraped >= max_scrape:
            break
        if article.source_type == "youtube":
            continue  # YouTube snippet is enough for Claude
        try:
            page = fetch_page(article.url, max_chars=max_chars)
            article.content = page.get("content", "")
            scraped += 1
        except Exception as e:
            logger.warning("Scrape failed for %s: %s", article.url, e)

    return articles


# â”€â”€ Claude summarization â”€â”€


def _build_prompt(kol: KolConfig, articles: list[KolArticle]) -> str:
    """Build Claude prompt for extracting KOL opinions."""
    texts = []
    for i, a in enumerate(articles, 1):
        body = a.content.strip() if a.content.strip() else a.snippet
        texts.append(f"[æ–‡ç« {i}] {a.title}\n{body[:4000]}")

    joined = "\n\n---\n\n".join(texts)
    return (
        f"ä»¥ä¸‹æ˜¯å¯èƒ½ä¸ {kol.name}ï¼ˆ{kol.title}ï¼‰ç›¸å…³çš„è¿‘æœŸæ–°é—»ã€‚\n\n"
        f"{joined}\n\n"
        f"ä»»åŠ¡ï¼šæç‚¼ {kol.name} æœ¬äººè¿‘æœŸçš„ 2-3 æ¡æ ¸å¿ƒè§‚ç‚¹æˆ–é‡è¦åŠ¨æ€ã€‚\n\n"
        f"ä¸¥æ ¼è§„åˆ™ï¼š\n"
        f"1. åªè¾“å‡º bullet pointï¼Œæ¯è¡Œä»¥ã€Œâ€¢ ã€å¼€å¤´ï¼Œæ¯æ¡ä¸è¶…è¿‡ 40 å­—\n"
        f"2. åªæå– {kol.name} æœ¬äººçš„è¨€è®ºã€å†³ç­–æˆ–ç›´æ¥ç›¸å…³çš„é‡å¤§äº‹ä»¶\n"
        f"3. ä¸æ­¤äººæ— å…³çš„æ–‡ç« ç›´æ¥å¿½ç•¥ï¼Œä¸è¦è§£é‡Šä¸ºä»€ä¹ˆå¿½ç•¥\n"
        f"4. å¦‚æœæ‰€æœ‰æ–‡ç« éƒ½ä¸ {kol.name} æ— å…³ï¼Œåªè¾“å‡ºï¼šæš‚æ— æ–°åŠ¨æ€\n"
        f"5. ç¦æ­¢è¾“å‡ºä»»ä½•è§£é‡Šã€è¯´æ˜ã€å‰ç¼€è¯­ã€‚åªæœ‰ bullet point æˆ–ã€Œæš‚æ— æ–°åŠ¨æ€ã€\n\n"
        f"ç”¨ä¸­æ–‡è¾“å‡ºã€‚"
    )


def summarize_kol_opinions(
    kol: KolConfig,
    articles: list[KolArticle],
    settings: dict,
) -> KolSummary:
    """Use Claude Haiku to distill core opinions."""
    summary = KolSummary(
        kol=kol,
        sources=[a.url for a in articles[:5] if a.url],
        source_types=[a.source_type for a in articles[:5] if a.url],
    )

    if not articles:
        summary.points = ["æš‚æ— æ–°åŠ¨æ€"]
        return summary

    # Build text for Claude
    prompt = _build_prompt(kol, articles)
    model = settings.get("claude_model", "claude-haiku-4-5-20251001")

    try:
        import anthropic
        client = anthropic.Anthropic()
        resp = client.messages.create(
            model=model,
            max_tokens=512,
            messages=[{"role": "user", "content": prompt}],
        )
        text = resp.content[0].text.strip()

        # Check for "no updates" response
        if "æš‚æ— æ–°åŠ¨æ€" in text and len(text) < 20:
            summary.points = ["æš‚æ— æ–°åŠ¨æ€"]
            return summary

        # Parse bullet points only â€” ignore non-bullet lines
        points = []
        for line in text.split("\n"):
            line = line.strip()
            if not line:
                continue
            # Only accept lines starting with bullet markers
            is_bullet = False
            for prefix in ("â€¢ ", "Â· ", "- ", "* ", "â€¢", "Â·"):
                if line.startswith(prefix):
                    line = line[len(prefix):].strip()
                    is_bullet = True
                    break
            if is_bullet and line and "æš‚æ— " not in line:
                points.append(line)

        summary.points = points[:settings.get("summary_max_points", 3)] if points else ["æš‚æ— æ–°åŠ¨æ€"]

    except Exception as e:
        logger.error("Claude summarization failed for %s: %s", kol.name, e)
        summary.error = str(e)
        # Fallback: use raw snippets
        summary.points = [
            a.snippet[:80] for a in articles[:2] if a.snippet
        ] or ["æš‚æ— æ–°åŠ¨æ€"]

    return summary


# â”€â”€ Orchestrator â”€â”€


def fetch_all_kol_summaries(
    kols: list[KolConfig],
    settings: dict,
) -> list[KolSummary]:
    """Fetch articles + summarize for all KOLs, sequentially."""
    from lib.web_search import WebSearchClient

    searcher = WebSearchClient()
    summaries: list[KolSummary] = []

    for kol in kols:
        logger.info("Processing KOL: %s", kol.name)
        try:
            articles = fetch_kol_articles(kol, searcher, settings)
            logger.info("  Found %d articles", len(articles))
            summary = summarize_kol_opinions(kol, articles, settings)
        except Exception as e:
            logger.error("KOL pipeline failed for %s: %s", kol.name, e)
            summary = KolSummary(kol=kol, points=["å¤„ç†å¤±è´¥"], error=str(e))
        summaries.append(summary)

    return summaries


# â”€â”€ Feishu Card Builder (Schema 2.0) â”€â”€

_CATEGORY_META = {
    "crypto": {"label": "Crypto", "icon": "ğŸª™", "color": "orange"},
    "tech":   {"label": "Tech",   "icon": "ğŸ’»", "color": "blue"},
}


def _kol_block(summary: KolSummary) -> list[dict]:
    """Build elements for one KOL â€” a visually distinct block."""
    kol = summary.kol
    meta = _CATEGORY_META.get(kol.category, {"label": "", "icon": "ğŸ‘¤", "color": "grey"})
    has_content = summary.points and summary.points != ["æš‚æ— æ–°åŠ¨æ€"]

    # Name row: icon + name + title tag
    name_line = f"**{kol.name}**ã€€<font color='{meta['color']}'>{kol.title}</font>"

    # Points
    if has_content:
        points_lines = []
        for p in summary.points:
            points_lines.append(f"â—¦ {p}")
        points_md = "\n".join(points_lines)
    else:
        points_md = "<font color='grey'>â€” æš‚æ— æ–°åŠ¨æ€ â€”</font>"

    # Source links (inline, subtle, with type icons)
    if has_content and summary.sources:
        link_parts = []
        types = summary.source_types or ["web"] * len(summary.sources)
        for i, (u, t) in enumerate(zip(summary.sources[:3], types[:3]), 1):
            icon = "â–¶" if t == "youtube" else str(i)
            link_parts.append(f"[{icon}]({u})")
        points_md += f"\n<font color='grey'>ğŸ“ {' Â· '.join(link_parts)}</font>"

    return [
        {"tag": "markdown", "content": name_line},
        {"tag": "markdown", "content": points_md},
    ]


def build_kol_card(summaries: list[KolSummary], date_str: str) -> dict:
    """Assemble Feishu Schema 2.0 card."""
    elements: list[dict] = []

    # Group by category
    groups: dict[str, list[KolSummary]] = {}
    for s in summaries:
        groups.setdefault(s.kol.category, []).append(s)

    # Render order: crypto â†’ tech â†’ other
    order = ["crypto", "tech"]
    ordered_keys = [k for k in order if k in groups] + [k for k in groups if k not in order]

    for idx, cat in enumerate(ordered_keys):
        group = groups[cat]
        meta = _CATEGORY_META.get(cat, {"label": cat.title(), "icon": "ğŸ“¡", "color": "grey"})

        if idx > 0:
            elements.append({"tag": "hr"})

        # Category header
        elements.append({
            "tag": "column_set",
            "flex_mode": "none",
            "background_style": "default",
            "columns": [{
                "tag": "column",
                "width": "weighted",
                "weight": 1,
                "elements": [{
                    "tag": "markdown",
                    "content": f"{meta['icon']}  **{meta['label']}**",
                }],
            }],
        })

        # Each KOL in this category
        for i, summary in enumerate(group):
            elements.extend(_kol_block(summary))
            # Light separator between KOLs in same category (not after last)
            if i < len(group) - 1:
                elements.append({"tag": "markdown", "content": " "})

    # Footer
    active_count = sum(
        1 for s in summaries
        if s.points and s.points != ["æš‚æ— æ–°åŠ¨æ€"]
    )
    elements.append({"tag": "hr"})
    elements.append({
        "tag": "markdown",
        "content": (
            f"<font color='grey'>"
            f"{date_str}ã€€Â·ã€€{active_count}/{len(summaries)} ä½æœ‰æ–°åŠ¨æ€ã€€Â·ã€€è´å¤šå¤š"
            f"</font>"
        ),
    })

    return {
        "schema": "2.0",
        "header": {
            "title": {
                "tag": "plain_text",
                "content": f"ğŸ¯ KOL è§‚ç‚¹é€Ÿé€’ Â· {date_str}",
            },
            "subtitle": {
                "tag": "plain_text",
                "content": "Daily KOL Briefing Â· è´å¤šå¤š",
            },
            "template": "violet",
        },
        "body": {
            "elements": elements,
        },
    }
